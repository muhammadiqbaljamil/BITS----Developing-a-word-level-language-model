{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement - 62 â€“ Natural Language Processing (NLP) Assignment\n",
    "\n",
    "## Group No - 26\n",
    "\n",
    "## Group Member Names:\n",
    "1. Sunil Mittal (BITS ID : 2021SC04968) - 100%\n",
    "2. Indira Saha (BITS ID : 2021SC04956) - 100%\n",
    "3. Vikram Panwar (BITS ID : 2021SC04958) - 100%\n",
    "4. Muhammad Iqbal J (BITS ID : 2021SC04960) - 100%\n",
    "\n",
    "<hr/>\n",
    "<p>\n",
    "Link to the Dataset: <a href=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\">Links</a> to an external site.\n",
    "\n",
    " Description of Data: This is a rich English word dataset. The main task is  Preparing text for developing a word-level language model. And then Train a neural network that contains an embedding and LSTM layer then used the learned model to generate new text with similar properties as the input text.\n",
    "\n",
    "<ol>\n",
    "<li>Define the above text in Python and encode the text as an integer. Determine the vocabulary size. Create the word sequence. 3</li>\n",
    "<li>Split the sequences into input (X) and output elements (y). fit your model to predict a probability distribution across all words in the vocabulary. 2</li>\n",
    "<li>Define and build the LSTM model for text generation.  3</li>\n",
    "<li>valuate the performance of the model. 2</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation :\n",
    "<p>\n",
    "Preparing text for developing a word-level language model involves several key steps:\n",
    "<p>\n",
    "<b>Text Gathering:</b> Gather the text data that you want to use for training your model. It could be a corpus of text from novels, newspapers, web pages, etc. It is essential to choose a corpus that is representative of the type of language model you want to develop.\n",
    "</p>\n",
    "<p>\n",
    "<b>Text Cleaning:</b> The raw text data usually contains a lot of noise like HTML tags, emojis, special characters, etc. that are not necessary for our language model. The text needs to be cleaned by removing these unnecessary characters.\n",
    "</p>\n",
    "<p>\n",
    "<b>Text Normalization:</b> This involves several steps such as:\n",
    "</p>\n",
    "<p>\n",
    "<b>Lowercasing:</b> To ensure that the model doesn't treat 'word' and 'Word' as two different words, it is a good idea to convert all the text into lowercase.\n",
    "</p>\n",
    "<p>\n",
    "<b>Lemmatization/Stemming:</b> This reduces the words to their base or root form. For instance, 'running' will be reduced to 'run'. However, whether you do this or not will depend on the specific requirements of your model.\n",
    "Removing Stop Words: Stop words like 'is', 'the', 'and' etc. occur very frequently in text data and don't contain valuable information, so they can be removed.\n",
    "Handling Punctuation: Depending on your needs, you may want to remove punctuation, or replace them with token representations.\n",
    "Tokenization: Tokenization is the process of breaking down the text into individual words or tokens. In a word-level language model, tokens are typically individual words.\n",
    "</p>\n",
    "<p>\n",
    "<b>Vocabulary Creation: After tokenization, a vocabulary of unique words is created. This vocabulary serves as the input feature space for the model.\n",
    "</p>\n",
    "<p>\n",
    "<b>Sequence Creation:</b> Language models are trained to predict the next word in a sequence. Therefore, from your tokenized text, you need to create sequences of words. The length of these sequences is a parameter that you can tune.\n",
    "</p>\n",
    "<p>\n",
    "<b>Encoding Sequences:</b> The sequences of words are then encoded into sequences of integers or one-hot encoded vectors. The encoding process transforms the textual information into numerical input that the language model can process.\n",
    "</p>\n",
    "<p>\n",
    "<b>Preparing Training and Validation Set:</b> Divide the dataset into a training set and validation set. The training set is used to train the model while the validation set is used to evaluate the model's performance during the training process.\n",
    "</p>\n",
    "<p>\n",
    "<b>Padding Sequences :</b> Depending on your model architecture, you may need to ensure that all sequences have the same length. You can do this by padding shorter sequences with a special symbol (like 'PAD').\n",
    "</p>\n",
    "<p>\n",
    "<b>Preparing Labels:</b> For each sequence, the corresponding label will be the word that comes after the sequence in the text data. These labels are what the model will try to predict.\n",
    "</p>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define the above text in Python and encode the text as an integer. Determine the vocabulary size. Create the word sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download lines are required to download the necessary resources for lemmatization and stopwords.\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Load the text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequence(all_words, seq_size):\n",
    "\n",
    "  for i in range(0, len(all_words), seq_size):\n",
    "    yield ' '.join(all_words[i:i + seq_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "print(\"preprocessing data...\")\n",
    "# Assume we have some text\n",
    "text=\"\"\n",
    "with open('nietzsche.txt', 'r') as file:\n",
    "    text_unprocessed = file.read()\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to clean and preprocess text\n",
    "#The preprocess_text function first converts the text to lowercase, then removes punctuation and\n",
    "#numbers using regular expressions. Next, it splits the text into individual words, lemmatizes each word\n",
    "#using NLTK's WordNet lemmatizer, and then joins the words back together. \n",
    "#The lemmatizer reduces each word to its base or root form (e.g., \"running\" becomes \"run\").\n",
    "# Contractions dictionary\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"womenthat\":\"women that\",\n",
    "\"womanwhat\":\"woman what\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text_unprocessed):\n",
    "    # Lowercase\n",
    "    text_unprocessed = text_unprocessed.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text_unprocessed = re.sub(r'[^\\w\\s]', '', text_unprocessed)\n",
    "    \n",
    "    # Replace contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "         text_unprocessed = text_unprocessed.replace(contraction, replacement)\n",
    "    \n",
    "    # Remove special characters\n",
    "    # \\W pattern in the regular expression matches any non-word character (equivalent to [^a-zA-Z0-9_]), \n",
    "    text_unprocessed = re.sub(r'\\W', ' ', text_unprocessed)\n",
    "    \n",
    "    # Remove emojis\n",
    "    # and the re.UNICODE flag makes the regular expression engine treat the input as a Unicode string.\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    text_unprocessed = RE_EMOJI.sub(r'', text_unprocessed)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text_unprocessed = re.sub(r'\\d+', '', text_unprocessed)\n",
    "    \n",
    "    # Lemmatize words\n",
    "    words = text_unprocessed.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    # print(words[:1000])\n",
    "    text_unprocessed = ' '.join(words)\n",
    "    \n",
    "    seq_len = 10\n",
    "    # print(len(words))\n",
    "    # print(type(words))\n",
    "    return list(create_sequence(words, seq_len))\n",
    "\n",
    "\n",
    "text = preprocess_text(text_unprocessed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Below code first tokenizes the input text, converting it into sequences of integers where each integer represents a unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the tokenizer on the text...\n",
      "\n",
      "Converting text into sequences of integers...\n",
      "\n",
      "Vocabulary size: 10404\n",
      "preface supposing\n",
      "preface supposing truth\n",
      "preface supposing truth woman\n",
      "preface supposing truth woman ground\n",
      "preface supposing truth woman ground suspecting\n",
      "preface supposing truth woman ground suspecting philosopher\n",
      "preface supposing truth woman ground suspecting philosopher far\n",
      "preface supposing truth woman ground suspecting philosopher far dogmatist\n",
      "preface supposing truth woman ground suspecting philosopher far dogmatist failed\n",
      "understand woman\n",
      "understand woman terrible\n",
      "understand woman terrible seriousness\n",
      "understand woman terrible seriousness clumsy\n",
      "understand woman terrible seriousness clumsy importunity\n",
      "understand woman terrible seriousness clumsy importunity usually\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess the text\n",
    "tokenizer = Tokenizer()\n",
    "print(\"Fitting the tokenizer on the text...\\n\")\n",
    "tokenizer.fit_on_texts(text)\n",
    "print(\"Converting text into sequences of integers...\\n\")\n",
    "# The vocabulary size is the number of unique words plus one\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "# Convert text into sequences of integers\n",
    "sequences = []\n",
    "for line in text:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "# Print out the first 15 sequences\n",
    "for seq in sequences[:15]:\n",
    "    print(' '.join([tokenizer.index_word[i] for i in seq]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split the sequences into input (X) and output elements (y). fit your model to predict a probability distribution across all words in the vocabulary:\n",
    "\n",
    "It creates input-output sequence pairs, where the model is trained to predict the next word given a sequence of previous words.\n",
    "It pads sequences to ensure they are of equal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sequences...\n",
      "\n",
      "Splitting sequences into input and output...\n",
      "\n",
      "X[0] (input sequence):  [   0    0    0    0    0    0    0    0    0 3089]\n",
      "y[0] (output word):  545\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences for equal input length \n",
    "print(\"Padding sequences...\\n\")\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "# Split sequences into input (X) and output (y)\n",
    "print(\"Splitting sequences into input and output...\\n\")\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Printing first sequence and its corresponding output\n",
    "print(\"X[0] (input sequence): \", X[0])\n",
    "print(\"y[0] (output word): \", np.argmax(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define and build the LSTM model for text generation:\n",
    "Constructs the LSTM model with an Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the LSTM model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build the LSTM model\n",
    "print(\"Building the LSTM model...\\n\")\n",
    "#Define Model\n",
    "model = Sequential()\n",
    "# Below Embedding layer converts the word integers into dense vectors of length 10. \n",
    "# The input dimension is the vocabulary size (the number of unique words plus one), \n",
    "# and the input length is the length of the input sequences.\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_sequence_len-1))\n",
    "#LSTM layer captures the sequence structure of the input. Here we're using 50 hidden units\n",
    "model.add(LSTM(50))\n",
    "#Dense layer outputs a probability distribution across all words. \n",
    "#The number of units is the vocabulary size, \n",
    "# and the softmax activation function ensures that the output is a probability distribution.\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Learning rate schedule function to optimize the model,It is to adjust the learning rate during training. \n",
    "# This can help to quickly converge during the initial stages of training and then slow down to finely tune the \n",
    "# model in the later stages.\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)\n",
    "\n",
    "callbacks = [\n",
    "    LearningRateScheduler(scheduler, verbose=1),\n",
    "    EarlyStopping(monitor='loss', patience=5),\n",
    "    ModelCheckpoint('best_model.h5', monitor='loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "#The model is compiled with the categorical cross entropy loss function, \n",
    "# #which is suitable for multi-class classification problems, and the Adam optimizer.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It trains this model on the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "\n",
      "Epoch 1/150\n",
      "1369/1369 - 31s - loss: 8.4604 - accuracy: 0.0122 - 31s/epoch - 23ms/step\n",
      "Epoch 2/150\n",
      "1369/1369 - 35s - loss: 8.0564 - accuracy: 0.0124 - 35s/epoch - 25ms/step\n",
      "Epoch 3/150\n",
      "1369/1369 - 31s - loss: 7.9526 - accuracy: 0.0125 - 31s/epoch - 23ms/step\n",
      "Epoch 4/150\n",
      "1369/1369 - 32s - loss: 7.8372 - accuracy: 0.0125 - 32s/epoch - 24ms/step\n",
      "Epoch 5/150\n",
      "1369/1369 - 32s - loss: 7.6809 - accuracy: 0.0129 - 32s/epoch - 24ms/step\n",
      "Epoch 6/150\n",
      "1369/1369 - 33s - loss: 7.4898 - accuracy: 0.0139 - 33s/epoch - 24ms/step\n",
      "Epoch 7/150\n",
      "1369/1369 - 33s - loss: 7.2786 - accuracy: 0.0155 - 33s/epoch - 24ms/step\n",
      "Epoch 8/150\n",
      "1369/1369 - 36s - loss: 7.0572 - accuracy: 0.0186 - 36s/epoch - 27ms/step\n",
      "Epoch 9/150\n",
      "1369/1369 - 35s - loss: 6.8328 - accuracy: 0.0243 - 35s/epoch - 25ms/step\n",
      "Epoch 10/150\n",
      "1369/1369 - 36s - loss: 6.6085 - accuracy: 0.0319 - 36s/epoch - 26ms/step\n",
      "Epoch 11/150\n",
      "1369/1369 - 36s - loss: 6.3864 - accuracy: 0.0450 - 36s/epoch - 26ms/step\n",
      "Epoch 12/150\n",
      "1369/1369 - 36s - loss: 6.1639 - accuracy: 0.0652 - 36s/epoch - 26ms/step\n",
      "Epoch 13/150\n",
      "1369/1369 - 38s - loss: 5.9447 - accuracy: 0.0861 - 38s/epoch - 28ms/step\n",
      "Epoch 14/150\n",
      "1369/1369 - 33s - loss: 5.7260 - accuracy: 0.1114 - 33s/epoch - 24ms/step\n",
      "Epoch 15/150\n",
      "1369/1369 - 33s - loss: 5.5134 - accuracy: 0.1383 - 33s/epoch - 24ms/step\n",
      "Epoch 16/150\n",
      "1369/1369 - 32s - loss: 5.3011 - accuracy: 0.1636 - 32s/epoch - 24ms/step\n",
      "Epoch 17/150\n",
      "1369/1369 - 32s - loss: 5.0937 - accuracy: 0.1917 - 32s/epoch - 24ms/step\n",
      "Epoch 18/150\n",
      "1369/1369 - 32s - loss: 4.8922 - accuracy: 0.2200 - 32s/epoch - 24ms/step\n",
      "Epoch 19/150\n",
      "1369/1369 - 32s - loss: 4.6939 - accuracy: 0.2453 - 32s/epoch - 24ms/step\n",
      "Epoch 20/150\n",
      "1369/1369 - 32s - loss: 4.5032 - accuracy: 0.2747 - 32s/epoch - 24ms/step\n",
      "Epoch 21/150\n",
      "1369/1369 - 35s - loss: 4.3189 - accuracy: 0.3002 - 35s/epoch - 25ms/step\n",
      "Epoch 22/150\n",
      "1369/1369 - 32s - loss: 4.1411 - accuracy: 0.3258 - 32s/epoch - 23ms/step\n",
      "Epoch 23/150\n",
      "1369/1369 - 32s - loss: 3.9724 - accuracy: 0.3496 - 32s/epoch - 24ms/step\n",
      "Epoch 24/150\n",
      "1369/1369 - 32s - loss: 3.8074 - accuracy: 0.3712 - 32s/epoch - 23ms/step\n",
      "Epoch 25/150\n",
      "1369/1369 - 32s - loss: 3.6542 - accuracy: 0.3946 - 32s/epoch - 24ms/step\n",
      "Epoch 26/150\n",
      "1369/1369 - 32s - loss: 3.5060 - accuracy: 0.4152 - 32s/epoch - 24ms/step\n",
      "Epoch 27/150\n",
      "1369/1369 - 32s - loss: 3.3660 - accuracy: 0.4365 - 32s/epoch - 23ms/step\n",
      "Epoch 28/150\n",
      "1369/1369 - 32s - loss: 3.2343 - accuracy: 0.4551 - 32s/epoch - 24ms/step\n",
      "Epoch 29/150\n",
      "1369/1369 - 32s - loss: 3.1083 - accuracy: 0.4744 - 32s/epoch - 23ms/step\n",
      "Epoch 30/150\n",
      "1369/1369 - 32s - loss: 2.9910 - accuracy: 0.4910 - 32s/epoch - 24ms/step\n",
      "Epoch 31/150\n",
      "1369/1369 - 33s - loss: 2.8750 - accuracy: 0.5084 - 33s/epoch - 24ms/step\n",
      "Epoch 32/150\n",
      "1369/1369 - 33s - loss: 2.7703 - accuracy: 0.5245 - 33s/epoch - 24ms/step\n",
      "Epoch 33/150\n",
      "1369/1369 - 32s - loss: 2.6671 - accuracy: 0.5408 - 32s/epoch - 23ms/step\n",
      "Epoch 34/150\n",
      "1369/1369 - 32s - loss: 2.5691 - accuracy: 0.5545 - 32s/epoch - 23ms/step\n",
      "Epoch 35/150\n",
      "1369/1369 - 32s - loss: 2.4798 - accuracy: 0.5693 - 32s/epoch - 23ms/step\n",
      "Epoch 36/150\n",
      "1369/1369 - 32s - loss: 2.3920 - accuracy: 0.5821 - 32s/epoch - 23ms/step\n",
      "Epoch 37/150\n",
      "1369/1369 - 32s - loss: 2.3084 - accuracy: 0.5953 - 32s/epoch - 23ms/step\n",
      "Epoch 38/150\n",
      "1369/1369 - 33s - loss: 2.2317 - accuracy: 0.6076 - 33s/epoch - 24ms/step\n",
      "Epoch 39/150\n",
      "1369/1369 - 32s - loss: 2.1590 - accuracy: 0.6183 - 32s/epoch - 24ms/step\n",
      "Epoch 40/150\n",
      "1369/1369 - 32s - loss: 2.0850 - accuracy: 0.6319 - 32s/epoch - 23ms/step\n",
      "Epoch 41/150\n",
      "1369/1369 - 33s - loss: 2.0188 - accuracy: 0.6441 - 33s/epoch - 24ms/step\n",
      "Epoch 42/150\n",
      "1369/1369 - 32s - loss: 1.9571 - accuracy: 0.6512 - 32s/epoch - 24ms/step\n",
      "Epoch 43/150\n",
      "1369/1369 - 34s - loss: 1.8949 - accuracy: 0.6611 - 34s/epoch - 25ms/step\n",
      "Epoch 44/150\n",
      "1369/1369 - 35s - loss: 1.8366 - accuracy: 0.6706 - 35s/epoch - 26ms/step\n",
      "Epoch 45/150\n",
      "1369/1369 - 33s - loss: 1.7810 - accuracy: 0.6809 - 33s/epoch - 24ms/step\n",
      "Epoch 46/150\n",
      "1369/1369 - 33s - loss: 1.7299 - accuracy: 0.6872 - 33s/epoch - 24ms/step\n",
      "Epoch 47/150\n",
      "1369/1369 - 33s - loss: 1.6830 - accuracy: 0.6968 - 33s/epoch - 24ms/step\n",
      "Epoch 48/150\n",
      "1369/1369 - 32s - loss: 1.6300 - accuracy: 0.7056 - 32s/epoch - 24ms/step\n",
      "Epoch 49/150\n",
      "1369/1369 - 33s - loss: 1.5882 - accuracy: 0.7117 - 33s/epoch - 24ms/step\n",
      "Epoch 50/150\n",
      "1369/1369 - 32s - loss: 1.5460 - accuracy: 0.7187 - 32s/epoch - 24ms/step\n",
      "Epoch 51/150\n",
      "1369/1369 - 32s - loss: 1.5008 - accuracy: 0.7262 - 32s/epoch - 24ms/step\n",
      "Epoch 52/150\n",
      "1369/1369 - 33s - loss: 1.4544 - accuracy: 0.7361 - 33s/epoch - 24ms/step\n",
      "Epoch 53/150\n",
      "1369/1369 - 32s - loss: 1.4228 - accuracy: 0.7394 - 32s/epoch - 24ms/step\n",
      "Epoch 54/150\n",
      "1369/1369 - 33s - loss: 1.3898 - accuracy: 0.7448 - 33s/epoch - 24ms/step\n",
      "Epoch 55/150\n",
      "1369/1369 - 34s - loss: 1.3473 - accuracy: 0.7524 - 34s/epoch - 25ms/step\n",
      "Epoch 56/150\n",
      "1369/1369 - 35s - loss: 1.3183 - accuracy: 0.7579 - 35s/epoch - 25ms/step\n",
      "Epoch 57/150\n",
      "1369/1369 - 37s - loss: 1.2860 - accuracy: 0.7626 - 37s/epoch - 27ms/step\n",
      "Epoch 58/150\n",
      "1369/1369 - 34s - loss: 1.2539 - accuracy: 0.7674 - 34s/epoch - 25ms/step\n",
      "Epoch 59/150\n",
      "1369/1369 - 31s - loss: 1.2258 - accuracy: 0.7721 - 31s/epoch - 23ms/step\n",
      "Epoch 60/150\n",
      "1369/1369 - 30s - loss: 1.1970 - accuracy: 0.7777 - 30s/epoch - 22ms/step\n",
      "Epoch 61/150\n",
      "1369/1369 - 31s - loss: 1.1669 - accuracy: 0.7830 - 31s/epoch - 23ms/step\n",
      "Epoch 62/150\n",
      "1369/1369 - 31s - loss: 1.1394 - accuracy: 0.7880 - 31s/epoch - 23ms/step\n",
      "Epoch 63/150\n",
      "1369/1369 - 35s - loss: 1.1184 - accuracy: 0.7909 - 35s/epoch - 25ms/step\n",
      "Epoch 64/150\n",
      "1369/1369 - 32s - loss: 1.0940 - accuracy: 0.7952 - 32s/epoch - 23ms/step\n",
      "Epoch 65/150\n",
      "1369/1369 - 32s - loss: 1.0699 - accuracy: 0.8010 - 32s/epoch - 24ms/step\n",
      "Epoch 66/150\n",
      "1369/1369 - 32s - loss: 1.0437 - accuracy: 0.8039 - 32s/epoch - 24ms/step\n",
      "Epoch 67/150\n",
      "1369/1369 - 32s - loss: 1.0222 - accuracy: 0.8082 - 32s/epoch - 23ms/step\n",
      "Epoch 68/150\n",
      "1369/1369 - 36s - loss: 1.0133 - accuracy: 0.8096 - 36s/epoch - 27ms/step\n",
      "Epoch 69/150\n",
      "1369/1369 - 38s - loss: 0.9818 - accuracy: 0.8146 - 38s/epoch - 28ms/step\n",
      "Epoch 70/150\n",
      "1369/1369 - 34s - loss: 0.9627 - accuracy: 0.8185 - 34s/epoch - 25ms/step\n",
      "Epoch 71/150\n",
      "1369/1369 - 32s - loss: 0.9483 - accuracy: 0.8202 - 32s/epoch - 23ms/step\n",
      "Epoch 72/150\n",
      "1369/1369 - 32s - loss: 0.9320 - accuracy: 0.8219 - 32s/epoch - 23ms/step\n",
      "Epoch 73/150\n",
      "1369/1369 - 32s - loss: 0.9116 - accuracy: 0.8266 - 32s/epoch - 23ms/step\n",
      "Epoch 74/150\n",
      "1369/1369 - 33s - loss: 0.8919 - accuracy: 0.8297 - 33s/epoch - 24ms/step\n",
      "Epoch 75/150\n",
      "1369/1369 - 32s - loss: 0.8851 - accuracy: 0.8307 - 32s/epoch - 23ms/step\n",
      "Epoch 76/150\n",
      "1369/1369 - 32s - loss: 0.8634 - accuracy: 0.8337 - 32s/epoch - 23ms/step\n",
      "Epoch 77/150\n",
      "1369/1369 - 32s - loss: 0.8508 - accuracy: 0.8365 - 32s/epoch - 23ms/step\n",
      "Epoch 78/150\n",
      "1369/1369 - 33s - loss: 0.8387 - accuracy: 0.8392 - 33s/epoch - 24ms/step\n",
      "Epoch 79/150\n",
      "1369/1369 - 32s - loss: 0.8210 - accuracy: 0.8419 - 32s/epoch - 23ms/step\n",
      "Epoch 80/150\n",
      "1369/1369 - 32s - loss: 0.8067 - accuracy: 0.8431 - 32s/epoch - 23ms/step\n",
      "Epoch 81/150\n",
      "1369/1369 - 32s - loss: 0.7865 - accuracy: 0.8482 - 32s/epoch - 23ms/step\n",
      "Epoch 82/150\n",
      "1369/1369 - 32s - loss: 0.7825 - accuracy: 0.8468 - 32s/epoch - 23ms/step\n",
      "Epoch 83/150\n",
      "1369/1369 - 32s - loss: 0.7752 - accuracy: 0.8483 - 32s/epoch - 23ms/step\n",
      "Epoch 84/150\n",
      "1369/1369 - 32s - loss: 0.7563 - accuracy: 0.8512 - 32s/epoch - 23ms/step\n",
      "Epoch 85/150\n",
      "1369/1369 - 34s - loss: 0.7448 - accuracy: 0.8526 - 34s/epoch - 25ms/step\n",
      "Epoch 86/150\n",
      "1369/1369 - 33s - loss: 0.7372 - accuracy: 0.8559 - 33s/epoch - 24ms/step\n",
      "Epoch 87/150\n",
      "1369/1369 - 32s - loss: 0.7227 - accuracy: 0.8568 - 32s/epoch - 23ms/step\n",
      "Epoch 88/150\n",
      "1369/1369 - 31s - loss: 0.7126 - accuracy: 0.8589 - 31s/epoch - 23ms/step\n",
      "Epoch 89/150\n",
      "1369/1369 - 32s - loss: 0.7009 - accuracy: 0.8612 - 32s/epoch - 23ms/step\n",
      "Epoch 90/150\n",
      "1369/1369 - 32s - loss: 0.6900 - accuracy: 0.8633 - 32s/epoch - 23ms/step\n",
      "Epoch 91/150\n",
      "1369/1369 - 32s - loss: 0.6794 - accuracy: 0.8657 - 32s/epoch - 23ms/step\n",
      "Epoch 92/150\n",
      "1369/1369 - 32s - loss: 0.6779 - accuracy: 0.8643 - 32s/epoch - 23ms/step\n",
      "Epoch 93/150\n",
      "1369/1369 - 32s - loss: 0.6688 - accuracy: 0.8656 - 32s/epoch - 23ms/step\n",
      "Epoch 94/150\n",
      "1369/1369 - 32s - loss: 0.6554 - accuracy: 0.8673 - 32s/epoch - 23ms/step\n",
      "Epoch 95/150\n",
      "1369/1369 - 32s - loss: 0.6438 - accuracy: 0.8698 - 32s/epoch - 23ms/step\n",
      "Epoch 96/150\n",
      "1369/1369 - 32s - loss: 0.6419 - accuracy: 0.8702 - 32s/epoch - 23ms/step\n",
      "Epoch 97/150\n",
      "1369/1369 - 32s - loss: 0.6290 - accuracy: 0.8708 - 32s/epoch - 23ms/step\n",
      "Epoch 98/150\n",
      "1369/1369 - 33s - loss: 0.6215 - accuracy: 0.8727 - 33s/epoch - 24ms/step\n",
      "Epoch 99/150\n",
      "1369/1369 - 33s - loss: 0.6148 - accuracy: 0.8741 - 33s/epoch - 24ms/step\n",
      "Epoch 100/150\n",
      "1369/1369 - 32s - loss: 0.6106 - accuracy: 0.8741 - 32s/epoch - 23ms/step\n",
      "Epoch 101/150\n",
      "1369/1369 - 32s - loss: 0.5990 - accuracy: 0.8778 - 32s/epoch - 23ms/step\n",
      "Epoch 102/150\n",
      "1369/1369 - 32s - loss: 0.6000 - accuracy: 0.8759 - 32s/epoch - 23ms/step\n",
      "Epoch 103/150\n",
      "1369/1369 - 32s - loss: 0.5895 - accuracy: 0.8772 - 32s/epoch - 23ms/step\n",
      "Epoch 104/150\n",
      "1369/1369 - 32s - loss: 0.5792 - accuracy: 0.8785 - 32s/epoch - 23ms/step\n",
      "Epoch 105/150\n",
      "1369/1369 - 32s - loss: 0.5762 - accuracy: 0.8806 - 32s/epoch - 24ms/step\n",
      "Epoch 106/150\n",
      "1369/1369 - 32s - loss: 0.5597 - accuracy: 0.8844 - 32s/epoch - 23ms/step\n",
      "Epoch 107/150\n",
      "1369/1369 - 32s - loss: 0.5636 - accuracy: 0.8819 - 32s/epoch - 24ms/step\n",
      "Epoch 108/150\n",
      "1369/1369 - 32s - loss: 0.5627 - accuracy: 0.8813 - 32s/epoch - 23ms/step\n",
      "Epoch 109/150\n",
      "1369/1369 - 32s - loss: 0.5519 - accuracy: 0.8829 - 32s/epoch - 23ms/step\n",
      "Epoch 110/150\n",
      "1369/1369 - 32s - loss: 0.5404 - accuracy: 0.8859 - 32s/epoch - 23ms/step\n",
      "Epoch 111/150\n",
      "1369/1369 - 32s - loss: 0.5429 - accuracy: 0.8844 - 32s/epoch - 23ms/step\n",
      "Epoch 112/150\n",
      "1369/1369 - 32s - loss: 0.5357 - accuracy: 0.8850 - 32s/epoch - 23ms/step\n",
      "Epoch 113/150\n",
      "1369/1369 - 32s - loss: 0.5300 - accuracy: 0.8864 - 32s/epoch - 23ms/step\n",
      "Epoch 114/150\n",
      "1369/1369 - 32s - loss: 0.5276 - accuracy: 0.8873 - 32s/epoch - 23ms/step\n",
      "Epoch 115/150\n",
      "1369/1369 - 32s - loss: 0.5183 - accuracy: 0.8879 - 32s/epoch - 23ms/step\n",
      "Epoch 116/150\n",
      "1369/1369 - 32s - loss: 0.5157 - accuracy: 0.8885 - 32s/epoch - 23ms/step\n",
      "Epoch 117/150\n",
      "1369/1369 - 32s - loss: 0.5078 - accuracy: 0.8895 - 32s/epoch - 23ms/step\n",
      "Epoch 118/150\n",
      "1369/1369 - 32s - loss: 0.5147 - accuracy: 0.8885 - 32s/epoch - 23ms/step\n",
      "Epoch 119/150\n",
      "1369/1369 - 32s - loss: 0.5050 - accuracy: 0.8899 - 32s/epoch - 23ms/step\n",
      "Epoch 120/150\n",
      "1369/1369 - 32s - loss: 0.4953 - accuracy: 0.8917 - 32s/epoch - 23ms/step\n",
      "Epoch 121/150\n",
      "1369/1369 - 32s - loss: 0.4971 - accuracy: 0.8909 - 32s/epoch - 23ms/step\n",
      "Epoch 122/150\n",
      "1369/1369 - 32s - loss: 0.4919 - accuracy: 0.8919 - 32s/epoch - 23ms/step\n",
      "Epoch 123/150\n",
      "1369/1369 - 32s - loss: 0.4778 - accuracy: 0.8955 - 32s/epoch - 23ms/step\n",
      "Epoch 124/150\n",
      "1369/1369 - 32s - loss: 0.4855 - accuracy: 0.8924 - 32s/epoch - 24ms/step\n",
      "Epoch 125/150\n",
      "1369/1369 - 32s - loss: 0.4754 - accuracy: 0.8954 - 32s/epoch - 24ms/step\n",
      "Epoch 126/150\n",
      "1369/1369 - 32s - loss: 0.4774 - accuracy: 0.8935 - 32s/epoch - 23ms/step\n",
      "Epoch 127/150\n",
      "1369/1369 - 32s - loss: 0.4742 - accuracy: 0.8944 - 32s/epoch - 23ms/step\n",
      "Epoch 128/150\n",
      "1369/1369 - 32s - loss: 0.4654 - accuracy: 0.8968 - 32s/epoch - 23ms/step\n",
      "Epoch 129/150\n",
      "1369/1369 - 32s - loss: 0.4666 - accuracy: 0.8954 - 32s/epoch - 23ms/step\n",
      "Epoch 130/150\n",
      "1369/1369 - 32s - loss: 0.4585 - accuracy: 0.8975 - 32s/epoch - 23ms/step\n",
      "Epoch 131/150\n",
      "1369/1369 - 32s - loss: 0.4569 - accuracy: 0.8979 - 32s/epoch - 23ms/step\n",
      "Epoch 132/150\n",
      "1369/1369 - 32s - loss: 0.4624 - accuracy: 0.8959 - 32s/epoch - 23ms/step\n",
      "Epoch 133/150\n",
      "1369/1369 - 32s - loss: 0.4497 - accuracy: 0.8985 - 32s/epoch - 23ms/step\n",
      "Epoch 134/150\n",
      "1369/1369 - 32s - loss: 0.4418 - accuracy: 0.9007 - 32s/epoch - 23ms/step\n",
      "Epoch 135/150\n",
      "1369/1369 - 32s - loss: 0.4406 - accuracy: 0.8999 - 32s/epoch - 23ms/step\n",
      "Epoch 136/150\n",
      "1369/1369 - 32s - loss: 0.4428 - accuracy: 0.8980 - 32s/epoch - 24ms/step\n",
      "Epoch 137/150\n",
      "1369/1369 - 32s - loss: 0.4436 - accuracy: 0.8993 - 32s/epoch - 23ms/step\n",
      "Epoch 138/150\n",
      "1369/1369 - 32s - loss: 0.4284 - accuracy: 0.9025 - 32s/epoch - 23ms/step\n",
      "Epoch 139/150\n",
      "1369/1369 - 32s - loss: 0.4260 - accuracy: 0.9030 - 32s/epoch - 23ms/step\n",
      "Epoch 140/150\n",
      "1369/1369 - 32s - loss: 0.4315 - accuracy: 0.9000 - 32s/epoch - 23ms/step\n",
      "Epoch 141/150\n",
      "1369/1369 - 32s - loss: 0.4267 - accuracy: 0.9000 - 32s/epoch - 23ms/step\n",
      "Epoch 142/150\n",
      "1369/1369 - 32s - loss: 0.4269 - accuracy: 0.9007 - 32s/epoch - 23ms/step\n",
      "Epoch 143/150\n",
      "1369/1369 - 32s - loss: 0.4206 - accuracy: 0.9023 - 32s/epoch - 23ms/step\n",
      "Epoch 144/150\n",
      "1369/1369 - 32s - loss: 0.4105 - accuracy: 0.9041 - 32s/epoch - 23ms/step\n",
      "Epoch 145/150\n",
      "1369/1369 - 32s - loss: 0.4136 - accuracy: 0.9043 - 32s/epoch - 23ms/step\n",
      "Epoch 146/150\n",
      "1369/1369 - 32s - loss: 0.4127 - accuracy: 0.9033 - 32s/epoch - 23ms/step\n",
      "Epoch 147/150\n",
      "1369/1369 - 32s - loss: 0.4069 - accuracy: 0.9049 - 32s/epoch - 23ms/step\n",
      "Epoch 148/150\n",
      "1369/1369 - 32s - loss: 0.4131 - accuracy: 0.9032 - 32s/epoch - 23ms/step\n",
      "Epoch 149/150\n",
      "1369/1369 - 32s - loss: 0.4127 - accuracy: 0.9029 - 32s/epoch - 23ms/step\n",
      "Epoch 150/150\n",
      "1369/1369 - 32s - loss: 0.3962 - accuracy: 0.9061 - 32s/epoch - 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ce2d8f2620>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Train the model\n",
    "print(\"Training the model...\\n\")\n",
    "#model is fitted on the input sequences X and their corresponding outputs y. \n",
    "#the number of epochs is set to 10, but can be increased to get higher accuracy\n",
    "model.fit(X, y, epochs=150, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, it uses the trained model to generate new text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate new text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        #print(\"Predicting word number {}...\\n\".format(_+1))\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted_probs, axis=-1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new text...\n",
      "\n",
      "But to speak seriously truth perception say much mutual sense man sinful know would like already something sought origin test presence said ideal laugh\n"
     ]
    }
   ],
   "source": [
    "# Generate new text\n",
    "print(\"\\nGenerating new text...\\n\")\n",
    "print(generate_text(\"But to speak seriously\", 20, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluate the performance of the model:\n",
    "\n",
    "After running the below evaluation, the result gives that the below gives a better performance.\n",
    "<ul>\n",
    "    <li>Optimizer: adam</li>\n",
    "    <li>Activation: softmax</li>\n",
    "</ul>\n",
    "\n",
    "For the detailed metrics please find the code execution below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building multiple LSTM model for comparison...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building multiple LSTM model for comparison...\\n\")\n",
    "class eval_result:\n",
    "    def __init__(self, opt, act, loss, accuracy):\n",
    "        self.opt = opt\n",
    "        self.act = act\n",
    "        self.loss = loss\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "def build_models_eval(optimizer, activation):\n",
    "    print(optimizer + ' ' + activation)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=10, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(len(tokenizer.word_index) + 1, activation=activation))\n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 5:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * math.exp(-0.1)\n",
    "\n",
    "    callbacks = [\n",
    "        LearningRateScheduler(scheduler, verbose=1),\n",
    "        EarlyStopping(monitor='loss', patience=5),\n",
    "        ModelCheckpoint('best_model.h5', monitor='loss', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training multiple models...\n",
      "\n",
      "adam tanh\n",
      "1369/1369 - 34s - loss: 11.0858 - accuracy: 0.0034 - 34s/epoch - 25ms/step\n",
      "1369/1369 [==============================] - 13s 9ms/step - loss: 13.1535 - accuracy: 0.0043\n",
      "adam softmax\n",
      "1369/1369 - 35s - loss: 8.4610 - accuracy: 0.0123 - 35s/epoch - 26ms/step\n",
      "1369/1369 [==============================] - 18s 13ms/step - loss: 8.1012 - accuracy: 0.0124\n",
      "adam relu\n",
      "1369/1369 - 38s - loss: 15.9577 - accuracy: 0.0019 - 38s/epoch - 28ms/step\n",
      "1369/1369 [==============================] - 12s 8ms/step - loss: 15.9574 - accuracy: 0.0038\n"
     ]
    }
   ],
   "source": [
    "print(\"Training multiple models...\\n\")\n",
    "eval_results = []\n",
    "res_compare = []\n",
    "# res_compare.append(eval_result('Optimizer', 'Activation', 'loss', 'accuracy'))\n",
    "\n",
    "# optimizers = ['adadelta', 'adagrad', 'adam']\n",
    "# activation = ['tanh', 'softmax', 'relu']\n",
    "optimizers = ['adam']\n",
    "activation = ['tanh', 'softmax', 'relu']\n",
    "\n",
    "for opt in optimizers:\n",
    "    for act in activation:\n",
    "        model = build_models_eval(opt, act)\n",
    "        #model is fitted on the input sequences X and their corresponding outputs y. \n",
    "        #the number of epochs is set to 10, but can be increased to get higher accuracy\n",
    "        model.fit(X, y, epochs=1, verbose=2)\n",
    "\n",
    "        res = model.evaluate(X, y)\n",
    "        # print(opt, act, res[0], res[1])\n",
    "        res_compare.append(eval_result(opt, act, res[0], res[1]))\n",
    "        eval_results.append([opt, act, res[0], res[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer:  adam\n",
      "Activation:  softmax\n",
      "Best accuracy:  1.2441501952707767 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "optimizer = \"\"\n",
    "acti = \"\"\n",
    "accuracy = 0\n",
    "\n",
    "for result in res_compare:\n",
    "    # print(result.opt, result.act, result.loss, result.accuracy)\n",
    "    if result.accuracy > accuracy:\n",
    "        accuracy = result.accuracy\n",
    "        optimizer = result.opt\n",
    "        acti = result.act\n",
    "\n",
    "print('Optimizer: ', optimizer)\n",
    "print('Activation: ', acti)\n",
    "print('Best accuracy: ', accuracy * 100, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79c792ad3c73468cd23fa5e462a9995a55a891b4cc8b43152f30c7a140787c39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
