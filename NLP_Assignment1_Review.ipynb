{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement - 62 â€“ NLP Assignment\n",
    "<hr/>\n",
    "<p>\n",
    "Link to the Dataset: <a href=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\">Links</a> to an external site.\n",
    "\n",
    " Description of Data: This is a rich English word dataset. The main task is  Preparing text for developing a word-level language model. And then Train a neural network that contains an embedding and LSTM layer then used the learned model to generate new text with similar properties as the input text.\n",
    "\n",
    "<ol>\n",
    "<li>Define the above text in Python and encode the text as an integer. Determine the vocabulary size. Create the word sequence. 3</li>\n",
    "<li>Split the sequences into input (X) and output elements (y). fit your model to predict a probability distribution across all words in the vocabulary. 2</li>\n",
    "<li>Define and build the LSTM model for text generation.  3</li>\n",
    "<li>valuate the performance of the model. 2</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation :\n",
    "<p>\n",
    "Preparing text for developing a word-level language model involves several key steps:\n",
    "<p>\n",
    "<b>Text Gathering:</b> Gather the text data that you want to use for training your model. It could be a corpus of text from novels, newspapers, web pages, etc. It is essential to choose a corpus that is representative of the type of language model you want to develop.\n",
    "</p>\n",
    "<p>\n",
    "<b>Text Cleaning:</b> The raw text data usually contains a lot of noise like HTML tags, emojis, special characters, etc. that are not necessary for our language model. The text needs to be cleaned by removing these unnecessary characters.\n",
    "</p>\n",
    "<p>\n",
    "<b>Text Normalization:</b> This involves several steps such as:\n",
    "</p>\n",
    "<p>\n",
    "<b>Lowercasing:</b> To ensure that the model doesn't treat 'word' and 'Word' as two different words, it is a good idea to convert all the text into lowercase.\n",
    "</p>\n",
    "<p>\n",
    "<b>Lemmatization/Stemming:</b> This reduces the words to their base or root form. For instance, 'running' will be reduced to 'run'. However, whether you do this or not will depend on the specific requirements of your model.\n",
    "Removing Stop Words: Stop words like 'is', 'the', 'and' etc. occur very frequently in text data and don't contain valuable information, so they can be removed.\n",
    "Handling Punctuation: Depending on your needs, you may want to remove punctuation, or replace them with token representations.\n",
    "Tokenization: Tokenization is the process of breaking down the text into individual words or tokens. In a word-level language model, tokens are typically individual words.\n",
    "</p>\n",
    "<p>\n",
    "<b>Vocabulary Creation: After tokenization, a vocabulary of unique words is created. This vocabulary serves as the input feature space for the model.\n",
    "</p>\n",
    "<p>\n",
    "<b>Sequence Creation:</b> Language models are trained to predict the next word in a sequence. Therefore, from your tokenized text, you need to create sequences of words. The length of these sequences is a parameter that you can tune.\n",
    "</p>\n",
    "<p>\n",
    "<b>Encoding Sequences:</b> The sequences of words are then encoded into sequences of integers or one-hot encoded vectors. The encoding process transforms the textual information into numerical input that the language model can process.\n",
    "</p>\n",
    "<p>\n",
    "<b>Preparing Training and Validation Set:</b> Divide the dataset into a training set and validation set. The training set is used to train the model while the validation set is used to evaluate the model's performance during the training process.\n",
    "</p>\n",
    "<p>\n",
    "<b>Padding Sequences :</b> Depending on your model architecture, you may need to ensure that all sequences have the same length. You can do this by padding shorter sequences with a special symbol (like 'PAD').\n",
    "</p>\n",
    "<p>\n",
    "<b>Preparing Labels:</b> For each sequence, the corresponding label will be the word that comes after the sequence in the text data. These labels are what the model will try to predict.\n",
    "</p>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>How to train a neural network that contains an embedding and LSTM layer then used the learned model to generate new text with similar properties as the input text.:</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Assuming that the text has been preprocessed and sequences of word indices have been created, here is how you can define and train a neural network:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "This will train a simple LSTM-based language model. You can make the model more complex and possibly improve performance by using more layers, dropout, recurrent dropout, etc.\n",
    "\n",
    "After training the model, you can use it to generate new text that has similar properties to the input text. Here's an example of how to do that:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a text of 100 words starting with \"Once upon a time\". The quality of the generated text will depend on how well the model has been trained.\n",
    "\n",
    "The function generate_text starts with some seed text, then predicts the next word, appends it to the text, and repeats this process as many times as specified. The pad_sequences function is used to ensure that the input sequence to the model always has the expected length.\n",
    "\n",
    "Please note that the preprocessing, the model's architecture, and its parameters may need to be tweaked to achieve good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download lines are required to download the necessary resources for lemmatization and stopwords.\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(list_a, chunk_size):\n",
    "\n",
    "  for i in range(0, len(list_a), chunk_size):\n",
    "    yield ' '.join(list_a[i:i + chunk_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assume we have some text\n",
    "text=\"\"\n",
    "with open('nietzsche.txt', 'r') as file:\n",
    "    text_unprocessed = file.read()\n",
    "    \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to clean and preprocess text\n",
    "#The preprocess_text function first converts the text to lowercase, then removes punctuation and\n",
    "#numbers using regular expressions. Next, it splits the text into individual words, lemmatizes each word\n",
    "#using NLTK's WordNet lemmatizer, and then joins the words back together. \n",
    "#The lemmatizer reduces each word to its base or root form (e.g., \"running\" becomes \"run\").\n",
    "# Contractions dictionary\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"womenthat\":\"women that\",\n",
    "\"womanwhat\":\"woman what\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text_unprocessed):\n",
    "    # text_unprocessed = text_unprocessed.replace('--', ' ')\n",
    "    \n",
    "    # Lowercase\n",
    "    text_unprocessed = text_unprocessed.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text_unprocessed = re.sub(r'[^\\w\\s]', '', text_unprocessed)\n",
    "    \n",
    "    # Replace contractions\n",
    "    for contraction, replacement in contractions.items():\n",
    "         text_unprocessed = text_unprocessed.replace(contraction, replacement)\n",
    "    \n",
    "    # Remove special characters\n",
    "    # \\W pattern in the regular expression matches any non-word character (equivalent to [^a-zA-Z0-9_]), \n",
    "    text_unprocessed = re.sub(r'\\W', ' ', text_unprocessed)\n",
    "    \n",
    "    # Remove emojis\n",
    "    # and the re.UNICODE flag makes the regular expression engine treat the input as a Unicode string.\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    text_unprocessed = RE_EMOJI.sub(r'', text_unprocessed)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text_unprocessed = re.sub(r'\\d+', '', text_unprocessed)\n",
    "    \n",
    "    # Lemmatize words\n",
    "    words = text_unprocessed.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    # print(words[:1000])\n",
    "    text_unprocessed = ' '.join(words)\n",
    "    \n",
    "    seq_len = 10\n",
    "    # print(len(words))\n",
    "    # print(type(words))\n",
    "    return list(split(words, seq_len))\n",
    "    \n",
    "    \n",
    "    # for i in range(seq_len, len(words)):\n",
    "    #     print(i)\n",
    "    #     seq = words[i-seq_len:i]\n",
    "    #     line = ' '.join(seq)\n",
    "    #     print(line)\n",
    "    #     processed_data.append(' '.join(seq))\n",
    "    \n",
    "    # return text_unprocessed\n",
    "    # return words\n",
    "\n",
    "# text = preprocess_text(text_unprocessed)\n",
    "# text = text_unprocessed\n",
    "print(\"1\")\n",
    "# print(text)\n",
    "\n",
    "text = preprocess_text(text_unprocessed)\n",
    "print(\"2\")\n",
    "# text = list()\n",
    "\n",
    "\n",
    "# for i in range(seq_len, len(words)):\n",
    "#     print(i)\n",
    "#     seq = words[i - seq_len:i]\n",
    "#     print(seq)\n",
    "#     text.append(' '.join(seq))\n",
    "\n",
    "print(len(text))\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code first tokenizes the input text, converting it into sequences of integers where each integer represents a unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the tokenizer on the text...\n",
      "\n",
      "Converting text into sequences of integers...\n",
      "\n",
      "Vocabulary size: 10404\n",
      "preface supposing\n",
      "preface supposing truth\n",
      "preface supposing truth woman\n",
      "preface supposing truth woman ground\n",
      "preface supposing truth woman ground suspecting\n",
      "preface supposing truth woman ground suspecting philosopher\n",
      "preface supposing truth woman ground suspecting philosopher far\n",
      "preface supposing truth woman ground suspecting philosopher far dogmatist\n",
      "preface supposing truth woman ground suspecting philosopher far dogmatist failed\n",
      "understand woman\n",
      "understand woman terrible\n",
      "understand woman terrible seriousness\n",
      "understand woman terrible seriousness clumsy\n",
      "understand woman terrible seriousness clumsy importunity\n",
      "understand woman terrible seriousness clumsy importunity usually\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Preprocess the text\n",
    "tokenizer = Tokenizer()\n",
    "print(\"Fitting the tokenizer on the text...\\n\")\n",
    "# print([text])\n",
    "tokenizer.fit_on_texts(text)\n",
    "print(\"Converting text into sequences of integers...\\n\")\n",
    "# The vocabulary size is the number of unique words plus one\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "# Convert text into sequences of integers\n",
    "sequences = []\n",
    "# for line in text.split('\\n'):\n",
    "#     print(line)\n",
    "#     print([line])\n",
    "#     token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "#     for i in range(1, len(token_list)):\n",
    "#         n_gram_sequence = token_list[:i+1]\n",
    "#         sequences.append(n_gram_sequence)\n",
    "\n",
    "for line in text:\n",
    "    # print(line)\n",
    "    # print([line])\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "# Print out the first 15 sequences\n",
    "for seq in sequences[:15]:\n",
    "    print(' '.join([tokenizer.index_word[i] for i in seq]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It creates input-output sequence pairs, where the model is trained to predict the next word given a sequence of previous words.\n",
    "It pads sequences to ensure they are of equal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sequences...\n",
      "\n",
      "11\n",
      "Splitting sequences into input and output...\n",
      "\n",
      "X[0] (input sequence):  [   0    0    0    0    0    0    0    0    0 3089]\n",
      "10\n",
      "y[0] (output word):  545\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences for equal input length \n",
    "print(\"Padding sequences...\\n\")\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "print(max_sequence_len)\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "# Split sequences into input (X) and output (y)\n",
    "print(\"Splitting sequences into input and output...\\n\")\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Printing first sequence and its corresponding output\n",
    "print(\"X[0] (input sequence): \", X[0])\n",
    "print(len(X[0]))\n",
    "print(\"y[0] (output word): \", np.argmax(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It constructs an LSTM model with an Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the LSTM model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build the LSTM model\n",
    "print(\"Building the LSTM model...\\n\")\n",
    "#Define Model\n",
    "model = Sequential()\n",
    "# Below Embedding layer converts the word integers into dense vectors of length 10. \n",
    "# The input dimension is the vocabulary size (the number of unique words plus one), \n",
    "# and the input length is the length of the input sequences.\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_sequence_len-1))\n",
    "#LSTM layer captures the sequence structure of the input. Here we're using 50 hidden units\n",
    "model.add(LSTM(50))\n",
    "#Dense layer outputs a probability distribution across all words. \n",
    "#The number of units is the vocabulary size, \n",
    "# and the softmax activation function ensures that the output is a probability distribution.\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Learning rate schedule function to optimize the model,It is to adjust the learning rate during training. \n",
    "# This can help to quickly converge during the initial stages of training and then slow down to finely tune the \n",
    "# model in the later stages.\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * math.exp(-0.1)\n",
    "\n",
    "callbacks = [\n",
    "    LearningRateScheduler(scheduler, verbose=1),\n",
    "    EarlyStopping(monitor='loss', patience=5),\n",
    "    ModelCheckpoint('best_model.h5', monitor='loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "#The model is compiled with the categorical cross entropy loss function, \n",
    "# #which is suitable for multi-class classification problems, and the Adam optimizer.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It trains this model on the sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "\n",
      "Epoch 1/10\n",
      "1369/1369 - 34s - loss: 8.4560 - accuracy: 0.0118 - 34s/epoch - 25ms/step\n",
      "Epoch 2/10\n",
      "1369/1369 - 32s - loss: 8.0583 - accuracy: 0.0124 - 32s/epoch - 23ms/step\n",
      "Epoch 3/10\n",
      "1369/1369 - 32s - loss: 7.9480 - accuracy: 0.0125 - 32s/epoch - 23ms/step\n",
      "Epoch 4/10\n",
      "1369/1369 - 35s - loss: 7.8279 - accuracy: 0.0127 - 35s/epoch - 25ms/step\n",
      "Epoch 5/10\n",
      "1369/1369 - 36s - loss: 7.6736 - accuracy: 0.0126 - 36s/epoch - 26ms/step\n",
      "Epoch 6/10\n",
      "1369/1369 - 33s - loss: 7.4846 - accuracy: 0.0132 - 33s/epoch - 24ms/step\n",
      "Epoch 7/10\n",
      "1369/1369 - 32s - loss: 7.2735 - accuracy: 0.0155 - 32s/epoch - 24ms/step\n",
      "Epoch 8/10\n",
      "1369/1369 - 32s - loss: 7.0548 - accuracy: 0.0185 - 32s/epoch - 23ms/step\n",
      "Epoch 9/10\n",
      "1369/1369 - 33s - loss: 6.8298 - accuracy: 0.0239 - 33s/epoch - 24ms/step\n",
      "Epoch 10/10\n",
      "1369/1369 - 34s - loss: 6.6040 - accuracy: 0.0322 - 34s/epoch - 25ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ebca576650>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Train the model\n",
    "print(\"Training the model...\\n\")\n",
    "#model is fitted on the input sequences X and their corresponding outputs y. \n",
    "#the number of epochs is set to 10, but can be increased to get higher accuracy\n",
    "model.fit(X, y, epochs=150, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, it uses the trained model to generate new text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate new text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        #print(\"Predicting word number {}...\\n\".format(_+1))\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted_probs, axis=-1)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating new text...\n",
      "\n",
      "But to speak seriously even still opposed german even color german possible exhaustion upon cloud surplus boat claim skin la panorama vessel pole veritable\n"
     ]
    }
   ],
   "source": [
    "# Generate new text\n",
    "print(\"\\nGenerating new text...\\n\")\n",
    "print(generate_text(\"But to speak seriously\", 20, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79c792ad3c73468cd23fa5e462a9995a55a891b4cc8b43152f30c7a140787c39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
