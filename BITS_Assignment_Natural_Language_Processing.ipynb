{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement - 62 â€“ NLP Assignment\n",
    "<hr/>\n",
    "<p>\n",
    "Link to the Dataset: <a href=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\">Links</a> to an external site.\n",
    "\n",
    " Description of Data: This is a rich English word dataset. The main task is  Preparing text for developing a word-level language model. And then Train a neural network that contains an embedding and LSTM layer then used the learned model to generate new text with similar properties as the input text.\n",
    "\n",
    "<ol>\n",
    "<li>Define the above text in Python and encode the text as an integer. Determine the vocabulary size. Create the word sequence. 3</li>\n",
    "<li>Split the sequences into input (X) and output elements (y). fit your model to predict a probability distribution across all words in the vocabulary. 2</li>\n",
    "<li>Define and build the LSTM model for text generation.  3</li>\n",
    "<li>valuate the performance of the model. 2</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation :\n",
    "<p>\n",
    "Preparing text for developing a word-level language model involves several key steps:\n",
    "<p>\n",
    "<b>Text Gathering:</b> Gather the text data that you want to use for training your model. It could be a corpus of text from novels, newspapers, web pages, etc. It is essential to choose a corpus that is representative of the type of language model you want to develop.\n",
    "</p>\n",
    "<p>\n",
    "<b>Text Cleaning:</b> The raw text data usually contains a lot of noise like HTML tags, emojis, special characters, etc. that are not necessary for our language model. The text needs to be cleaned by removing these unnecessary characters.\n",
    "</p>\n",
    "<p>\n",
    "<b>Text Normalization:</b> This involves several steps such as:\n",
    "</p>\n",
    "<p>\n",
    "<b>Lowercasing:</b> To ensure that the model doesn't treat 'word' and 'Word' as two different words, it is a good idea to convert all the text into lowercase.\n",
    "</p>\n",
    "<p>\n",
    "<b>Lemmatization/Stemming:</b> This reduces the words to their base or root form. For instance, 'running' will be reduced to 'run'. However, whether you do this or not will depend on the specific requirements of your model.\n",
    "Removing Stop Words: Stop words like 'is', 'the', 'and' etc. occur very frequently in text data and don't contain valuable information, so they can be removed.\n",
    "Handling Punctuation: Depending on your needs, you may want to remove punctuation, or replace them with token representations.\n",
    "Tokenization: Tokenization is the process of breaking down the text into individual words or tokens. In a word-level language model, tokens are typically individual words.\n",
    "</p>\n",
    "<p>\n",
    "<b>Vocabulary Creation: After tokenization, a vocabulary of unique words is created. This vocabulary serves as the input feature space for the model.\n",
    "</p>\n",
    "<p>\n",
    "<b>Sequence Creation:</b> Language models are trained to predict the next word in a sequence. Therefore, from your tokenized text, you need to create sequences of words. The length of these sequences is a parameter that you can tune.\n",
    "</p>\n",
    "<p>\n",
    "<b>Encoding Sequences:</b> The sequences of words are then encoded into sequences of integers or one-hot encoded vectors. The encoding process transforms the textual information into numerical input that the language model can process.\n",
    "</p>\n",
    "<p>\n",
    "<b>Preparing Training and Validation Set:</b> Divide the dataset into a training set and validation set. The training set is used to train the model while the validation set is used to evaluate the model's performance during the training process.\n",
    "</p>\n",
    "<p>\n",
    "<b>Padding Sequences :</b> Depending on your model architecture, you may need to ensure that all sequences have the same length. You can do this by padding shorter sequences with a special symbol (like 'PAD').\n",
    "</p>\n",
    "<p>\n",
    "<b>Preparing Labels:</b> For each sequence, the corresponding label will be the word that comes after the sequence in the text data. These labels are what the model will try to predict.\n",
    "</p>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>How to train a neural network that contains an embedding and LSTM layer then used the learned model to generate new text with similar properties as the input text.:</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Assuming that the text has been preprocessed and sequences of word indices have been created, here is how you can define and train a neural network:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "vocab_size = len(word_to_index) + 1  # size of your vocabulary\n",
    "embedding_dim = 100  # dimension of the embedding space\n",
    "sequence_length = 50  # length of your sequences\n",
    "num_units = 256  # number of units in LSTM layer\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_length),\n",
    "    LSTM(num_units),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Assuming you have input sequences `x_train` and one-hot encoded target sequences `y_train`\n",
    "model.fit(x_train, y_train, epochs=50, verbose=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "This will train a simple LSTM-based language model. You can make the model more complex and possibly improve performance by using more layers, dropout, recurrent dropout, etc.\n",
    "\n",
    "After training the model, you can use it to generate new text that has similar properties to the input text. Here's an example of how to do that:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = [word_to_index[word] for word in seed_text.split()]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')  # pad the sequence\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted_probs, axis=-1)  # get the most probable next word\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in word_to_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "seed_text = \"Once upon a time\"\n",
    "next_words = 100\n",
    "\n",
    "print(generate_text(seed_text, next_words, model, sequence_length))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a text of 100 words starting with \"Once upon a time\". The quality of the generated text will depend on how well the model has been trained.\n",
    "\n",
    "The function generate_text starts with some seed text, then predicts the next word, appends it to the text, and repeats this process as many times as specified. The pad_sequences function is used to ensure that the input sequence to the model always has the expected length.\n",
    "\n",
    "Please note that the preprocessing, the model's architecture, and its parameters may need to be tweaked to achieve good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume we have some text\n",
    "with open('nietzsche.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Preprocess the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text into sequences of integers\n",
    "sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adadad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences for equal input length \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdadad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sequences into input (X) and output (y)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erwrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2886/2886 - 62s - loss: 6.7160 - accuracy: 0.0732 - 62s/epoch - 21ms/step\n",
      "Epoch 2/100\n",
      "2886/2886 - 56s - loss: 6.3315 - accuracy: 0.0944 - 56s/epoch - 19ms/step\n",
      "Epoch 3/100\n",
      "2886/2886 - 57s - loss: 6.1012 - accuracy: 0.1084 - 57s/epoch - 20ms/step\n",
      "Epoch 4/100\n",
      "2886/2886 - 62s - loss: 5.9327 - accuracy: 0.1225 - 62s/epoch - 21ms/step\n",
      "Epoch 5/100\n",
      "2886/2886 - 62s - loss: 5.7764 - accuracy: 0.1324 - 62s/epoch - 22ms/step\n",
      "Epoch 6/100\n",
      "2886/2886 - 59s - loss: 5.6414 - accuracy: 0.1398 - 59s/epoch - 20ms/step\n",
      "Epoch 7/100\n",
      "2886/2886 - 59s - loss: 5.5202 - accuracy: 0.1462 - 59s/epoch - 20ms/step\n",
      "Epoch 8/100\n",
      "2886/2886 - 62s - loss: 5.4077 - accuracy: 0.1520 - 62s/epoch - 21ms/step\n",
      "Epoch 9/100\n",
      "2886/2886 - 65s - loss: 5.3010 - accuracy: 0.1582 - 65s/epoch - 23ms/step\n",
      "Epoch 10/100\n",
      "2886/2886 - 71s - loss: 5.1996 - accuracy: 0.1627 - 71s/epoch - 25ms/step\n",
      "Epoch 11/100\n",
      "2886/2886 - 78s - loss: 5.1043 - accuracy: 0.1660 - 78s/epoch - 27ms/step\n",
      "Epoch 12/100\n",
      "2886/2886 - 80s - loss: 5.0128 - accuracy: 0.1703 - 80s/epoch - 28ms/step\n",
      "Epoch 13/100\n",
      "2886/2886 - 79s - loss: 4.9251 - accuracy: 0.1745 - 79s/epoch - 27ms/step\n",
      "Epoch 14/100\n",
      "2886/2886 - 186s - loss: 4.8426 - accuracy: 0.1789 - 186s/epoch - 64ms/step\n",
      "Epoch 15/100\n",
      "2886/2886 - 57s - loss: 4.7633 - accuracy: 0.1837 - 57s/epoch - 20ms/step\n",
      "Epoch 16/100\n",
      "2886/2886 - 59s - loss: 4.6870 - accuracy: 0.1883 - 59s/epoch - 20ms/step\n",
      "Epoch 17/100\n",
      "2886/2886 - 57s - loss: 4.6138 - accuracy: 0.1932 - 57s/epoch - 20ms/step\n",
      "Epoch 18/100\n",
      "2886/2886 - 57s - loss: 4.5435 - accuracy: 0.1987 - 57s/epoch - 20ms/step\n",
      "Epoch 19/100\n",
      "2886/2886 - 62s - loss: 4.4765 - accuracy: 0.2044 - 62s/epoch - 21ms/step\n",
      "Epoch 20/100\n",
      "2886/2886 - 88s - loss: 4.4133 - accuracy: 0.2102 - 88s/epoch - 31ms/step\n",
      "Epoch 21/100\n",
      "2886/2886 - 69s - loss: 4.3527 - accuracy: 0.2154 - 69s/epoch - 24ms/step\n",
      "Epoch 22/100\n",
      "2886/2886 - 50s - loss: 4.2957 - accuracy: 0.2215 - 50s/epoch - 17ms/step\n",
      "Epoch 23/100\n",
      "2886/2886 - 71s - loss: 4.2409 - accuracy: 0.2279 - 71s/epoch - 25ms/step\n",
      "Epoch 24/100\n",
      "2886/2886 - 73s - loss: 4.1896 - accuracy: 0.2330 - 73s/epoch - 25ms/step\n",
      "Epoch 25/100\n",
      "2886/2886 - 49s - loss: 4.1397 - accuracy: 0.2387 - 49s/epoch - 17ms/step\n",
      "Epoch 26/100\n",
      "2886/2886 - 64s - loss: 4.0928 - accuracy: 0.2444 - 64s/epoch - 22ms/step\n",
      "Epoch 27/100\n",
      "2886/2886 - 78s - loss: 4.0492 - accuracy: 0.2503 - 78s/epoch - 27ms/step\n",
      "Epoch 28/100\n",
      "2886/2886 - 52s - loss: 4.0071 - accuracy: 0.2545 - 52s/epoch - 18ms/step\n",
      "Epoch 29/100\n",
      "2886/2886 - 64s - loss: 3.9666 - accuracy: 0.2600 - 64s/epoch - 22ms/step\n",
      "Epoch 30/100\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train the model\n",
    "model.fit(X, y, epochs=100, verbose=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate new text\n",
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new text\n",
    "print(generate_text(\"Once upon a time\", 20, model, max_sequence_len))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script does the following:\n",
    "\n",
    "It first tokenizes the input text, converting it into sequences of integers where each integer represents a unique word.\n",
    "It creates input-output sequence pairs, where the model is trained to predict the next word given a sequence of previous words.\n",
    "It pads sequences to ensure they are of equal length.\n",
    "It constructs an LSTM model with an Embedding layer.\n",
    "It trains this model on the sequence data.\n",
    "Finally, it uses the trained model to generate new text.\n",
    "Note: For simplicity, I've kept the model's architecture and the preprocessing steps relatively straightforward. A larger dataset and more complex model architecture (more layers, regularization techniques like dropout, etc.) can potentially yield better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79c792ad3c73468cd23fa5e462a9995a55a891b4cc8b43152f30c7a140787c39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
